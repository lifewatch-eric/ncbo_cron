#!/usr/bin/env ruby

# Exit cleanly from an early interrupt
Signal.trap("INT") { exit 1 }

# Setup the bundled gems in our environment
require 'bundler/setup'

# Configure the process for the current cron configuration.
require_relative '../lib/ncbo_cron'
config_exists = File.exist?(File.expand_path('../../config/config.rb', __FILE__))
abort("Please create a config/config.rb file using the config/config.rb.sample as a template") unless config_exists
require_relative '../config/config'

platform = "local"
if LinkedData.settings.goo_host.include? "stage"
  platform = "stage"
elsif LinkedData.settings.goo_host.include? "prod"
  platform = "prod"
end
puts "Running on #{platform} platform"

require 'uri'
require 'benchmark'
require 'optparse'

options = {all: false}
opt_parser = OptionParser.new do |opts|
  # Set a banner, displayed at the top of the help screen.
  #opts.banner = "Usage: ncbo_ontology_index [options]"

  opts.on('-c', '--solr-core-url URL', 'Optional URL of the Solr core to be used for indexing. Ex: http://localhost:8983/solr/core1') do |url|
    options[:solr_core_url] = url
  end

  options[:ontologies] = []
  opts.on('-a', '--all-ontologies', 'Index all ontologies (this or -o option required).') do
    options[:solr_core_url] = NcboCron.settings.search_index_all_url if (options[:solr_core_url].nil?)

    LinkedData::Models::Ontology.all.each  do |ont|
      ont.bring(:acronym)
      options[:ontologies].push(ont.acronym)
    end
    options[:all] = true
  end

  opts.on('-o', '--ontologies ACRONYM1,ACRONYM2,ACRONYM3', 'Comma-separated list of ontologies to index (this or -a option required).') do |acronyms|
    options[:ontologies] = acronyms.split(",").map {|o| o.strip}
  end

  opts.on('-z', '--optimize [true/false]', 'Whether to optimize the index after the indexing completion. Default: true') do |optimize|
    options[:optimize] = optimize
  end

  options[:logfile] = STDOUT
  opts.on( '-l', '--logfile FILE', "Write log to FILE (default is STDOUT)" ) do |filename|
    options[:logfile] = filename
  end

  #options[:verbose] = false
  #opts.on( '-v', '--verbose', 'Output more information' ) do
  #  options[:verbose] = true
  #end

  # Display the help screen, all programs are assumed to have this option.
  opts.on( '-h', '--help', 'Display this screen' ) do
    puts opts
    exit
  end
end

# Parse the command-line. The 'parse' method simply parses ARGV, while the 'parse!' method parses ARGV and removes
# any options found there, as well as any parameters for the options.
opt_parser.parse!
unless options[:ontologies]
  puts opt_parser.help
  exit(1)
end
options[:solr_core_url] = LinkedData.settings.search_server_url if (options[:solr_core_url].nil?)

if options[:optimize].nil? || options[:optimize] != "false"
  options[:optimize] = true
else
  options[:optimize] = false
end

def index_ontology(indexed_ontologies, acronym, logger)
  begin
    ont = LinkedData::Models::Ontology.find(acronym).first

    if ont.nil?
      logger.info("Ontology is nil: #{acronym}. Retrying...")
      sleep(2)
      ont = LinkedData::Models::Ontology.find(acronym).first
      raise "Ontology not found even after retrying: #{acronym}" if ont.nil?
    end
    logger.info("Retrieved ontology: #{acronym}")

    sub = ont.latest_submission(status: :rdf)
    if sub.nil?
      raise "Cannot find latest submission with 'RDF' parsed status for ontology: #{acronym}"
    end
    # NOTE: process_submission controls all aspects of re-indexing, including
    # removal of the old index and committing a new index.  There is no 'dry-run' option.
    NcboCron::Models::OntologySubmissionParser.new.process_submission(logger, sub.id.to_s, {index_search: true})
    indexed_ontologies << acronym
  rescue Exception => e
    msg = "Exception: #{e.class}: #{e.message}\n#{e.backtrace.join("\n")}"
    puts msg
    logger.error(msg)
  end
end

def valid_url?(url)
  url = URI.parse(url) rescue false
  url.kind_of?(URI::HTTP) || url.kind_of?(URI::HTTPS)
end

abort("The Solr core URL you provided is invalid. Aborting...\n\n") unless valid_url?(options[:solr_core_url])

if options[:all] && options[:solr_core_url].downcase == LinkedData.settings.search_server_url.downcase
  puts "WARNING: You are about to clear the main core search index on #{options[:solr_core_url]}!!! This will affect ALL searches!!! Are you sure you mean to do this?"
elsif options[:all]
  puts "You are about to clear the index on #{options[:solr_core_url]} and kick off a lengthy re-indexing process. Are you sure?"
end

=begin
if options[:all]
  puts "Type 'yes' to continue: "
  $stdout.flush
  confirm = $stdin.gets
  abort("Aborting...\n\n") unless (confirm.strip == 'yes')
=end

begin
  logger = Logger.new(options[:logfile])
  puts "Processing details are logged to #{options[:logfile] == STDOUT ? "STDOUT" : options[:logfile]}"
  msg = ""

  if options[:all]
    msg = "Processing index for all ontologies on #{options[:solr_core_url]}"
  else
    msg = "Processing index for ontologies: #{options[:ontologies]} on #{options[:solr_core_url]}"
  end
  puts msg
  logger.info(msg)

  Goo.configure do |conf|
    conf.add_search_backend(:main, service: options[:solr_core_url])
  end

  indexed_ontologies = []
  remaining_ontologies = options[:ontologies].length

  time = Benchmark.realtime do
    logger.info("There is a total of #{options[:ontologies].length} ontolog#{options[:ontologies].length > 1 ? "ies" : "y"} to index")

    if options[:all]
      logger.info("Clearing existing index on #{options[:solr_core_url]}")
      LinkedData::Models::Class.indexClear()
      LinkedData::Models::Class.indexCommit()
    end

    options[:ontologies].each do |acronym|
      index_ontology(indexed_ontologies, acronym, logger)
      remaining_ontologies -= 1
      logger.info("There is a total of #{remaining_ontologies} ontolog#{remaining_ontologies > 1 ? "ies" : "y"} remaining out of #{options[:ontologies].length} ontologies") if (remaining_ontologies > 0)
    end

    if options[:optimize]
      logger.info("Optimizing index...")
      logger.flush
      t0 = Time.now
      LinkedData::Models::Class.indexOptimize()
      logger.info("Completed optimizing index in #{Time.now - t0} sec.")
      logger.flush
    end
  end

  if options[:all]
    msg = "Completed processing index for all ontologies on #{options[:solr_core_url]} in #{(time/60).round(1)} minutes."
  else
    msg = "Completed processing index for ontologies: #{indexed_ontologies} on #{options[:solr_core_url]} in #{(time/60).round(1)} minutes."
  end
  puts msg
  logger.info(msg)
rescue Exception => e
  msg = "Failed indexing with exception: #{e.class}: #{e.message}\n#{e.backtrace.join("\n")}"
  logger.error(msg)
  puts msg
  exit(1)
end
